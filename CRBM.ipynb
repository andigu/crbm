{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CRBM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM7ktDcSb1bE6Apc6ZxA9xs",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andigu/crbm/blob/main/CRBM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_dqRVtqKSKh"
      },
      "source": [
        "!pip install qiskit emcee numdifftools pyscf > /dev/null\n",
        "import emcee\n",
        "import tensorflow as tf \n",
        "import tensorflow_probability as tfp\n",
        "import numpy as np \n",
        "from sklearn.neural_network import BernoulliRBM\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import imageio\n",
        "from datetime import datetime\n",
        "import os\n",
        "from qiskit.quantum_info import Statevector, Pauli, Operator\n",
        "from qiskit.visualization import plot_bloch_multivector, plot_bloch_vector, plot_histogram, plot_state_paulivec\n",
        "from qiskit.visualization.utils import _bloch_multivector_data\n",
        "from tqdm.keras import TqdmCallback\n",
        "from scipy import optimize as opt\n",
        "import numdifftools as nd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZZJr1g1pGmY"
      },
      "source": [
        "from qiskit.aqua.algorithms import VQE, NumPyEigensolver\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from qiskit.chemistry.components.variational_forms import UCCSD\n",
        "from qiskit.chemistry.components.initial_states import HartreeFock\n",
        "from qiskit.circuit.library import EfficientSU2\n",
        "from qiskit.aqua.components.optimizers import COBYLA, SPSA, SLSQP\n",
        "from qiskit.aqua.operators import Z2Symmetries\n",
        "from qiskit import IBMQ, BasicAer, Aer\n",
        "from qiskit.chemistry.drivers import PySCFDriver, UnitsType\n",
        "from qiskit.chemistry import FermionicOperator\n",
        "from qiskit import IBMQ\n",
        "from qiskit.aqua.operators import Z2Symmetries\n",
        "\n",
        "def get_qubit_op(dist):\n",
        "    driver = PySCFDriver(atom=f'H .0 .0 .0; H .0 .0 {dist}', \n",
        "                         unit=UnitsType.ANGSTROM, charge=0, spin=0, basis='sto3g')\n",
        "    molecule = driver.run()\n",
        "    num_particles = molecule.num_alpha + molecule.num_beta\n",
        "    qubitOp = FermionicOperator(h1=molecule.one_body_integrals, h2=molecule.two_body_integrals).mapping(map_type='parity')\n",
        "    qubitOp = Z2Symmetries.two_qubit_reduction(qubitOp, num_particles)\n",
        "    return qubitOp.to_opflow(), molecule.nuclear_repulsion_energy\n",
        "\n",
        "tmp = []\n",
        "ds = np.linspace(0.5, 3, 20)\n",
        "for dist in ds:\n",
        "    op, repulsion = get_qubit_op(dist)\n",
        "    tmp.append(np.linalg.eigvalsh(op.to_matrix()).min() + repulsion)\n",
        "plt.plot(ds, tmp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4DxsXLXM_wM"
      },
      "source": [
        "import itertools\n",
        "\n",
        "class CRBM(tf.keras.models.Model):\n",
        "    def __init__(self, nv=1, nh=5, seed=0):\n",
        "        np.random.seed(seed)\n",
        "        tf.random.set_seed(seed)\n",
        "        super(CRBM, self).__init__()\n",
        "        self.nv, self.nh = nv, nh\n",
        "        self.bv = tf.Variable(initial_value=CRBM.random_normal((nv,1)), dtype=tf.dtypes.complex64, trainable=True)\n",
        "        self.bh = tf.Variable(initial_value=CRBM.random_normal((nh,1)), dtype=tf.dtypes.complex64, trainable=True)\n",
        "        self.W = tf.Variable(initial_value=CRBM.random_normal((nv,nh)), dtype=tf.dtypes.complex64, trainable=True)\n",
        "        \n",
        "        v_vecs = np.array([list(\"{0:b}\".format(x).zfill(nv)) for x in np.arange(2**nv)]).astype(np.int)\n",
        "        self.v_vecs = tf.cast(tf.convert_to_tensor(v_vecs), tf.dtypes.complex64)\n",
        "    \n",
        "    @staticmethod\n",
        "    def random_normal(shape, scale=1, loc=0):\n",
        "        return np.random.normal(size=shape, scale=scale, loc=loc) + 1j*np.random.normal(size=shape, scale=scale, loc=loc)\n",
        "    \n",
        "    def psi(self, v):\n",
        "        \"\"\"\n",
        "        Accepts only length N binary strings\n",
        "        \"\"\"\n",
        "        if tf.rank(v) == 1:\n",
        "            v = tf.expand_dims(v, axis=-1)\n",
        "        elif tf.rank(v) == 2:\n",
        "            v = tf.transpose(v)\n",
        "        bv, bh, W = self.bv, self.bh, self.W\n",
        "        ret = tf.linalg.adjoint(bv) @ v + tf.reduce_sum(tf.math.log(tf.math.conj(tf.exp(bh + tf.linalg.adjoint(W) @ v)) + 1), axis=0)\n",
        "        return tf.transpose(tf.exp(ret))\n",
        "\n",
        "    def prob(self, measurements):\n",
        "        psi_vec = self.psi(self.v_vecs)\n",
        "        psi_vec = psi_vec / tf.linalg.norm(psi_vec)\n",
        "        psi_proj = tf.math.conj(measurements) @ psi_vec\n",
        "        return tf.math.abs(psi_proj) ** 2\n",
        "\n",
        "    def call(self, measurements):\n",
        "        self.add_loss(-tf.math.reduce_mean(tf.math.log(self.prob(measurements))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqY6OHN-NuOc"
      },
      "source": [
        "operator = get_qubit_op(0.7)[0].to_matrix()\n",
        "N = 2\n",
        "psi = np.linalg.eigh(operator)[1][:,0]\n",
        "sv = Statevector(psi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuipKM-vzHrX"
      },
      "source": [
        "print(get_qubit_op(1)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vsx9Jlb2NnNz"
      },
      "source": [
        "from tensorflow.keras.callbacks import Callback\n",
        "class EarlyStoppingByLoss(Callback):\n",
        "    def __init__(self, monitor='val_loss', value=0.00001, verbose=0):\n",
        "        super(Callback, self).__init__()\n",
        "        self.monitor = monitor\n",
        "        self.value = value\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        current = logs.get(self.monitor)\n",
        "        if current < self.value:\n",
        "            if self.verbose > 0:\n",
        "                print(\"Epoch %05d: early stopping THR\" % epoch)\n",
        "            self.model.stop_training = True\n",
        "\n",
        "def measure(psi, n_sample=100, basis='Z'*N):\n",
        "    mat = Pauli.from_label(basis).to_matrix()\n",
        "    eigs, basis = np.linalg.eigh(mat) # basis[:,i] is ith basis vec\n",
        "    probs = np.abs(np.conj(basis.T) @ psi)**2\n",
        "    basis = basis.astype(np.complex64)\n",
        "    idx = np.random.choice(2**N, size=n_sample, p=probs)\n",
        "    return basis.T[idx]\n",
        "\n",
        "def train(psi, n_sample, seed=0):\n",
        "    np.random.seed(seed)\n",
        "    measurements = np.concatenate((\n",
        "        measure(psi, n_sample, basis='II'), \n",
        "        measure(psi, n_sample, basis='IZ'), \n",
        "        measure(psi, n_sample, basis='ZI'), \n",
        "        measure(psi, n_sample, basis='ZZ'), \n",
        "        measure(psi, n_sample, basis='XX'), \n",
        "    ), axis=0)\n",
        "\n",
        "    bs = 20\n",
        "    crbm = CRBM(nv=N, nh=4)\n",
        "    crbm.compile(optimizer=tf.optimizers.Adam(1e-1))\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='loss', verbose=False, save_best_only=True),\n",
        "        # TqdmCallback(batch_size=bs),\n",
        "        EarlyStoppingByLoss(monitor='loss', value=0.3)\n",
        "    ]\n",
        "    hist = crbm.fit(measurements, verbose=False, epochs=1000, callbacks=callbacks, batch_size=bs, shuffle=True)\n",
        "    crbm.load_weights(\"model.h5\")\n",
        "    crbm.compile(optimizer=tf.optimizers.RMSprop(1e-2))\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='loss', verbose=False, save_best_only=True),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(patience=100, monitor='loss', factor=0.3),\n",
        "        # TqdmCallback(batch_size=bs),\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='loss', patience=200, restore_best_weights=True)\n",
        "    ]\n",
        "    hist = crbm.fit(measurements, verbose=False, epochs=4000, callbacks=callbacks, batch_size=len(measurements), shuffle=False)\n",
        "    crbm.load_weights(\"model.h5\")\n",
        "\n",
        "    psi2 = crbm.psi(crbm.v_vecs).numpy().flatten()\n",
        "    psi2 = psi2/psi2[0]\n",
        "    print(n_sample, seed, min(hist.history['loss']), len(hist.history['loss']))\n",
        "    return psi2/np.linalg.norm(psi2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwA68ZChdDFQ"
      },
      "source": [
        "from tqdm.notebook import trange\n",
        "res = []\n",
        "x = np.logspace(2.5, 4.5, num=8).astype(int)\n",
        "x = (x//20) * 20\n",
        "for n_sample in x:\n",
        "    res.append([])\n",
        "    for i in trange(100):\n",
        "        sv2 = Statevector(train(psi, n_sample, seed=i*n_sample))\n",
        "        res[-1].append(sv2.expectation_value(operator))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MNkUmXuqVpl"
      },
      "source": [
        "ops = get_qubit_op(0.7)[0]\n",
        "expect = np.real(np.array([sv.expectation_value(op.to_matrix())/op.coeff for op in ops]))\n",
        "coeffs = np.array([op.coeff for op in ops])\n",
        "vars = np.sum(coeffs**2 * (1-expect**2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXYHYWMThYD1"
      },
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "y = np.var(np.real(res), axis=1)\n",
        "plt.loglog(x,y, 'o', label='RBM')\n",
        "plt.loglog(x,vars/x, 'o', label='Averaging (QC)')\n",
        "plt.xlabel(\"Number of Samples\")\n",
        "plt.ylabel(\"Variance of observable\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.grid(which='minor', ls='--', alpha=0.5)\n",
        "plt.savefig(\"rbm-results.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFGhdvUpyWnM"
      },
      "source": [
        "plt.figure(figsize=(8,5), dpi=100)\n",
        "y = np.mean((np.real(res) - np.linalg.eigvalsh(operator)[0])**2, axis=1)\n",
        "plt.loglog(x,y/0.0016**2, 'o', label='RBM')\n",
        "plt.loglog(x,(vars/x)/0.0016**2, 'o', label='Naive Averaging')\n",
        "plt.xlabel(\"Number of Samples\")\n",
        "plt.ylabel(r\"Mean squared error $\\langle (\\frac{\\tilde{O} - O}{\\epsilon})^2 \\rangle$\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.grid(which='minor', ls='--', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"rbm-results.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGdEwyfgomjb"
      },
      "source": [
        "eps = 0.0016\n",
        "fig, (ax0, ax1) = plt.subplots(2, 1, sharex=True, figsize=(8,8))\n",
        "ax0.loglog(x, np.mean(np.real(res) - np.linalg.eigvalsh(operator)[0], axis=1)/eps, 'o')\n",
        "ax0.set_ylabel(r\"Bias (normalized by $\\epsilon$)\")\n",
        "ax0.grid()\n",
        "ax0.grid(which='minor', ls='--', alpha=0.5)\n",
        "ax1.loglog(x, np.std(np.real(res), axis=1)/eps, 'o')\n",
        "ax1.set_ylabel(r\"Standard dev (normalized by $\\epsilon$)\")\n",
        "ax1.grid()\n",
        "ax1.grid(which='minor', ls='--', alpha=0.5)\n",
        "ax1.set_xlabel(\"Number of samples\")\n",
        "fig.tight_layout()\n",
        "fig.savefig(\"bias.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flZW34i5mayn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}